{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMlIOz3kfs9CtWNsXp3Pllq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"eSfgrRyRY04I"},"outputs":[],"source":["# Full pipeline script: EDA → ML → DL → Transformers → LIME/SHAP\n","# Requirements (install if needed):\n","# pip install pandas numpy matplotlib seaborn scikit-learn gensim tensorflow transformers sentence-transformers lime shap xgboost\n","\n","import os\n","import random\n","import math\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix, roc_curve, auc\n","from sklearn.pipeline import Pipeline\n","from sklearn.base import BaseEstimator, TransformerMixin\n","\n","# ML models\n","from sklearn.svm import SVC\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier\n","from xgboost import XGBClassifier\n","\n","# NLP & embeddings\n","import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('averaged_perceptron_tagger')\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk import pos_tag\n","from nltk.stem import WordNetLemmatizer, PorterStemmer\n","\n","from gensim.models import Word2Vec, FastText\n","from sentence_transformers import SentenceTransformer\n","\n","# Deep learning\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, GRU, Dense, Dropout, Conv1D, GlobalMaxPool1D\n","\n","# Interpretability\n","from lime.lime_text import LimeTextExplainer\n","import shap\n","\n","# Reproducibility\n","SEED = 42\n","random.seed(SEED)\n","np.random.seed(SEED)\n","tf.random.set_seed(SEED)\n","\n","# ---------------------------\n","# CONFIG\n","# ---------------------------\n","DATA_PATH = \"/content/English News dataset.csv\"\n","RUN_DL = False\n","TRAIN_TRANSFORMER = False\n","MAX_TFIDF_FEATURES = 2000\n","TEST_SIZE = 0.2\n","BATCH_SIZE = 32\n","EPOCHS = 3\n","\n","# ---------------------------\n","# 1. LOAD + BASIC CLEAN\n","# ---------------------------\n","df = pd.read_csv(DATA_PATH)\n","df.columns = [c.strip().lower() for c in df.columns]\n","\n","# adjust column names if different\n","if 'headline' not in df.columns:\n","    # attempt to find likely column\n","    for cand in ['text', 'title', 'headline_text', 'news']:\n","        if cand in df.columns:\n","            df.rename(columns={cand: 'headline'}, inplace=True)\n","            break\n","if 'labels' not in df.columns and 'label' in df.columns:\n","    df.rename(columns={'label': 'labels'}, inplace=True)\n","\n","assert 'headline' in df.columns and 'labels' in df.columns, \"Dataset must contain 'headline' and 'labels' columns.\"\n","\n","df['headline'] = df['headline'].astype(str).str.strip()\n","df['labels'] = df['labels'].astype(str).str.strip().str.lower()\n","df = df[df['labels'].isin(['clickbait', 'non-clickbait'])]  # keep only these labels\n","\n","print(\"Loaded dataset:\", df.shape)\n","print(df['labels'].value_counts())\n","\n","# ---------------------------\n","# 2. EDA: Data distribution + Headline length analysis\n","# ---------------------------\n","df['headline_len_chars'] = df['headline'].str.len()\n","df['headline_len_tokens'] = df['headline'].apply(lambda x: len(word_tokenize(x)))\n","\n","plt.figure(figsize=(8,4))\n","sns.countplot(x='labels', data=df)\n","plt.title(\"Class distribution\")\n","plt.show()\n","\n","plt.figure(figsize=(12,4))\n","sns.histplot(data=df, x='headline_len_tokens', hue='labels', bins=30, kde=True, element='step')\n","plt.title(\"Headline length (tokens) distribution by class\")\n","plt.show()\n","\n","# Show a small table of lengths\n","print(df.groupby('labels')[['headline_len_chars','headline_len_tokens']].describe().T)\n","\n","# ---------------------------\n","# 3. PREPROCESSING utilities\n","# ---------------------------\n","stop_words = set(stopwords.words('english'))\n","lemmatizer = WordNetLemmatizer()\n","stemmer = PorterStemmer()\n","\n","def preprocess_text(text, do_lower=True, remove_digits=True, remove_special=True):\n","    if not isinstance(text, str):\n","        return \"\"\n","    t = text\n","    if do_lower:\n","        t = t.lower()\n","    if remove_digits:\n","        t = ''.join(ch for ch in t if not ch.isdigit())\n","    if remove_special:\n","        t = ''.join(ch if (ch.isalnum() or ch.isspace()) else ' ' for ch in t)\n","    tokens = word_tokenize(t)\n","    tokens = [tok for tok in tokens if tok not in stop_words and len(tok)>1]\n","    tokens = [lemmatizer.lemmatize(tok) for tok in tokens]\n","    tokens = [stemmer.stem(tok) for tok in tokens]\n","    return \" \".join(tokens)\n","\n","# apply preprocessing (you can cache/skip if already done)\n","df['clean'] = df['headline'].apply(preprocess_text)\n","\n","# POS tags for each preprocessed headline\n","def pos_sequence(text):\n","    toks = word_tokenize(text)\n","    tags = pos_tag(toks)\n","    return \" \".join([tag for _, tag in tags])\n","\n","df['pos'] = df['clean'].apply(pos_sequence)\n","\n","# tokens list for embedding training\n","df['tokens'] = df['clean'].apply(lambda x: x.split())\n","\n","# ---------------------------\n","# 4. Statistical Significance tests (real data)\n","# compute TF-IDF mean per headline and run tests comparing classes\n","# ---------------------------\n","vectorizer_small = TfidfVectorizer(max_features=500, stop_words='english')\n","X_small = vectorizer_small.fit_transform(df['headline']).toarray()\n","df['tfidf_mean'] = X_small.mean(axis=1)\n","df['label_bin'] = df['labels'].map({'clickbait':1,'non-clickbait':0})\n","\n","click = df[df['label_bin']==1]['tfidf_mean'].values\n","nonclick = df[df['label_bin']==0]['tfidf_mean'].values\n","\n","from scipy import stats\n","t_stat, t_p = stats.ttest_ind(click, nonclick, equal_var=False)\n","anova_stat, anova_p = stats.f_oneway(click, nonclick)\n","# chi-square: make binned counts\n","bins = 10\n","click_hist, _ = np.histogram(click, bins=bins)\n","nonclick_hist, _ = np.histogram(nonclick, bins=bins)\n","chi2_stat, chi2_p, _, _ = stats.chi2_contingency([click_hist+1e-8, nonclick_hist+1e-8])\n","# z-test:\n","mean_diff = click.mean() - nonclick.mean()\n","pooled_std = math.sqrt(click.var()/len(click) + nonclick.var()/len(nonclick))\n","z_stat = mean_diff / pooled_std\n","from scipy.stats import norm\n","z_p = 2*(1 - norm.cdf(abs(z_stat)))\n","\n","print(\"T-test p:\", t_p, \"ANOVA p:\", anova_p, \"Chi2 p:\", chi2_p, \"Z-test p:\", z_p)\n","\n","# plot log p-values (for visual clarity take -log10)\n","pvals = {'T-Test': t_p, 'ANOVA': anova_p, 'Chi-Square': chi2_p, 'Z-Test': z_p}\n","logp = {k: -np.log10(v) if v>0 else 50 for k,v in pvals.items()}  # cap if zero\n","plt.figure(figsize=(6,4))\n","sns.barplot(x=list(logp.keys()), y=list(logp.values()), palette=\"Blues_r\")\n","plt.ylabel(\"-log10(p-value)\")\n","plt.title(\"Statistical significance (-log10 p) for Clickbait vs Non-Clickbait\")\n","plt.show()\n","\n","# ---------------------------\n","# 5. FEATURE ENGINEERING + ML MODELING (TF-IDF, POS, N-gram)\n","# We'll run a short set of classifiers and report metrics\n","# ---------------------------\n","LABEL = 'label_bin'\n","X = df['clean']\n","X_pos = df['pos']\n","y = df[LABEL].values\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=SEED, stratify=y)\n","Xpos_train, Xpos_test, _, _ = train_test_split(X_pos, y, test_size=TEST_SIZE, random_state=SEED, stratify=y)\n","\n","# vectorizers\n","tfidf = TfidfVectorizer(max_features=MAX_TFIDF_FEATURES, ngram_range=(1,1))\n","tfidf_ngram = CountVectorizer(ngram_range=(2,3), max_features=2000)  # ngram features\n","tfidf_pos = TfidfVectorizer(max_features=1000)  # POS tag tfidf\n","\n","# models dictionary (smaller set for speed; you can expand)\n","models = {\n","    'SVM': SVC(probability=True, random_state=SEED),\n","    'LR': LogisticRegression(max_iter=1000, random_state=SEED),\n","    'DT': DecisionTreeClassifier(random_state=SEED),\n","    'KNN': KNeighborsClassifier(),\n","    'GB': GradientBoostingClassifier(random_state=SEED),\n","    'RF': RandomForestClassifier(n_estimators=100, random_state=SEED),\n","    'XGB': XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=SEED),\n","    'AdaBoost': AdaBoostClassifier(random_state=SEED)\n","}\n","\n","def eval_and_print(y_true, y_pred):\n","    acc = accuracy_score(y_true, y_pred)\n","    prec = precision_score(y_true, y_pred, zero_division=0)\n","    rec = recall_score(y_true, y_pred, zero_division=0)\n","    f1 = f1_score(y_true, y_pred, zero_division=0)\n","    return acc, prec, rec, f1\n","\n","results = []\n","\n","# helper to train pipeline quickly\n","def run_feature_models(feature_type):\n","    if feature_type == 'TFIDF':\n","        vec = tfidf\n","        Xtr = X_train\n","        Xte = X_test\n","    elif feature_type == 'POS':\n","        vec = tfidf_pos\n","        Xtr = Xpos_train\n","        Xte = Xpos_test\n","    elif feature_type == 'NGRAM':\n","        vec = tfidf_ngram\n","        Xtr = X_train\n","        Xte = X_test\n","    else:\n","        raise ValueError\n","\n","    Xtr_v = vec.fit_transform(Xtr)\n","    Xte_v = vec.transform(Xte)\n","\n","    for name, model in models.items():\n","        print(f\"Training {name} on {feature_type} ...\")\n","        model.fit(Xtr_v, y_train)\n","        ypred = model.predict(Xte_v)\n","        acc, prec, rec, f1 = eval_and_print(y_test, ypred)\n","        print(f\"{feature_type} | {name} -> acc:{acc:.4f} prec:{prec:.4f} rec:{rec:.4f} f1:{f1:.4f}\")\n","        results.append((feature_type, name, acc, prec, rec, f1))\n","    print(\"-- done\", feature_type)\n","\n","for feat in ['TFIDF','POS','NGRAM']:\n","    run_feature_models(feat)\n","\n","results_df = pd.DataFrame(results, columns=['Feature','Model','Accuracy','Precision','Recall','F1'])\n","print(results_df.sort_values(by='Accuracy', ascending=False).head(15))\n","\n","# Example: compute ROC for one model (SVM with TF-IDF)\n","svm_pipeline = Pipeline([('tfidf', tfidf), ('clf', SVC(probability=True, random_state=SEED))])\n","svm_pipeline.fit(X_train, y_train)\n","probs = svm_pipeline.predict_proba(X_test)[:,1]\n","fpr, tpr, _ = roc_curve(y_test, probs)\n","roc_auc = auc(fpr, tpr)\n","plt.figure()\n","plt.plot(fpr, tpr, label=f\"SVM TFIDF (AUC={roc_auc:.3f})\")\n","plt.plot([0,1],[0,1],'k--')\n","plt.legend()\n","plt.title(\"ROC Curve\")\n","plt.show()\n","\n","# ---------------------------\n","# 6. DEEP LEARNING: Word2Vec / FastText embeddings -> LSTM, Bi-LSTM, GRU, Bi-GRU\n","# NOTE: training deep models is time-consuming. Toggle RUN_DL to True to train; otherwise code prepares data and model definitions.\n","# ---------------------------\n","# Tokenize and sequences\n","tokenizer = Tokenizer(num_words=20000, oov_token=\"<OOV>\")\n","tokenizer.fit_on_texts(df['tokens'].apply(lambda x: \" \".join(x)))\n","sequences = tokenizer.texts_to_sequences(df['tokens'].apply(lambda x: \" \".join(x)))\n","maxlen = max(len(s) for s in sequences)\n","X_seq = pad_sequences(sequences, maxlen=maxlen, padding='post')\n","y_seq = df['label_bin'].values\n","\n","# train/val split\n","Xtr_seq, Xte_seq, ytr_seq, yte_seq = train_test_split(X_seq, y_seq, test_size=TEST_SIZE, random_state=SEED, stratify=y_seq)\n","\n","# Train Word2Vec / FastText on tokens (small vectors for speed)\n","w2v_size = 100\n","w2v_model = Word2Vec(df['tokens'].tolist(), vector_size=w2v_size, window=5, min_count=1, workers=4, seed=SEED)\n","ft_model = FastText(df['tokens'].tolist(), vector_size=w2v_size, window=5, min_count=1, workers=4, seed=SEED)\n","\n","# build embedding matrix\n","word_index = tokenizer.word_index\n","vocab_size = min(20000, len(word_index))+1\n","embedding_matrix_w2v = np.zeros((vocab_size, w2v_size))\n","embedding_matrix_ft = np.zeros((vocab_size, w2v_size))\n","for word, i in word_index.items():\n","    if i>=vocab_size: continue\n","    if word in w2v_model.wv:\n","        embedding_matrix_w2v[i] = w2v_model.wv[word]\n","    if word in ft_model.wv:\n","        embedding_matrix_ft[i] = ft_model.wv[word]\n","\n","def build_lstm(embedding_matrix, bidir=False, units=128, dropout=0.3):\n","    model = Sequential()\n","    emb_layer = Embedding(input_dim=embedding_matrix.shape[0], output_dim=embedding_matrix.shape[1],\n","                          weights=[embedding_matrix], input_length=maxlen, trainable=False)\n","    model.add(emb_layer)\n","    if bidir:\n","        model.add(Bidirectional(LSTM(units, return_sequences=False)))\n","    else:\n","        model.add(LSTM(units))\n","    model.add(Dropout(dropout))\n","    model.add(Dense(64, activation='relu'))\n","    model.add(Dropout(0.2))\n","    model.add(Dense(1, activation='sigmoid'))\n","    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    return model\n","\n","# Quick training if requested\n","if RUN_DL:\n","    print(\"Training LSTM (Word2Vec)...\")\n","    model_lstm = build_lstm(embedding_matrix_w2v, bidir=False, units=64)\n","    model_lstm.fit(Xtr_seq, ytr_seq, validation_split=0.1, epochs=EPOCHS, batch_size=32)\n","    print(\"Eval:\", model_lstm.evaluate(Xte_seq, yte_seq))\n","\n","# ---------------------------\n","# 7. TRANSFORMERS (inference using pretrained fine-tuned / or quick inference)\n","# We'll use pretrained sequence-classification models (if available) that are fine-tuned on similar tasks\n","# For speed we will use the pipeline API where possible\n","# ---------------------------\n","from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n","\n","# choose model name that is small & fine-tuned (if you have a custom model, load that)\n","model_names = {\n","    'bert': 'distilbert-base-uncased-finetuned-sst-2-english',   # sentiment model as proxy\n","    'distilbert': 'distilbert-base-uncased-finetuned-sst-2-english',\n","    'roberta-large': 'roberta-large-mnli',\n","    't5': 'google/t5-v1_1-base'  # T5 not for classification by default\n","}\n","\n","# Example: use distilbert pipeline for fast inference (class names will be 'LABEL_0','LABEL_1' for some models)\n","pipe = pipeline(\"text-classification\", model=model_names['distilbert'], device=0 if tf.config.list_physical_devices('GPU') else -1, return_all_scores=True)\n","\n","# apply to a handful or to test set (careful with large datasets)\n","sample_texts = df['headline'].sample(200, random_state=SEED).tolist()\n","preds = pipe(sample_texts)\n","# convert to clickbait probability (approx): depends on model label order - here we just show example\n","print(\"Transformer sample predictions (example):\", preds[:3])\n","\n","# ---------------------------\n","# 8. LIME and SHAP\n","#   - LIME: explain a classifier (we'll use the RF model trained earlier as example)\n","#   - SHAP: summary for tree model (e.g., RandomForest)\n","# ---------------------------\n","# Ensure we have a trained RF on TF-IDF earlier (we trained 'RF' in models; reuse vectorizer tfidf and model)\n","rf_model = RandomForestClassifier(n_estimators=100, random_state=SEED)\n","Xtr_tfidf = tfidf.fit_transform(X_train)\n","Xte_tfidf = tfidf.transform(X_test)\n","rf_model.fit(Xtr_tfidf, y_train)\n","\n","# LIME for text (explain one test sample)\n","class_names = ['non-clickbait','clickbait']\n","explainer = LimeTextExplainer(class_names=class_names)\n","idx = 5\n","exp = explainer.explain_instance(X_test.iloc[idx], rf_model.predict_proba, num_features=6, vectorizer=tfidf)\n","print(\"LIME explanation (as list):\", exp.as_list())\n","# show HTML in notebook: exp.show_in_notebook(text=True)\n","\n","# SHAP for RandomForest\n","explainer_shap = shap.TreeExplainer(rf_model)\n","# shap value needs numeric representation (tfidf); sample subset for speed\n","Xshap = Xte_tfidf[:100].toarray()\n","shap_values = explainer_shap.shap_values(Xshap)\n","# summary plot (in notebook)\n","# shap.summary_plot(shap_values, features=Xshap, feature_names=tfidf.get_feature_names_out())\n","\n","print(\"Done. You can visualize LIME/SHAP plots in notebook environment.\")\n","\n","# ---------------------------\n","# 9. ERROR ANALYSIS: FP / FN by headline length (requires predictions)\n","#    We'll use the RF model predictions on X_test\n","# ---------------------------\n","y_pred = rf_model.predict(Xte_tfidf)\n","test_df = pd.DataFrame({'headline': X_test, 'true': y_test, 'pred': y_pred})\n","test_df['headline_len_tokens'] = test_df['headline'].apply(lambda x: len(word_tokenize(x)))\n","test_df['error_type'] = test_df.apply(lambda r: 'TP' if (r.true==1 and r.pred==1) else ('TN' if (r.true==0 and r.pred==0) else ('FP' if (r.true==0 and r.pred==1) else 'FN')), axis=1)\n","\n","plt.figure(figsize=(8,4))\n","sns.boxplot(x='error_type', y='headline_len_tokens', data=test_df, order=['FP','FN','TN','TP'])\n","plt.title(\"Headline length distribution by error type\")\n","plt.show()\n","\n","# statistical tests across FP vs FN lengths\n","fp_len = test_df[test_df['error_type']=='FP']['headline_len_tokens'].values\n","fn_len = test_df[test_df['error_type']=='FN']['headline_len_tokens'].values\n","\n","if len(fp_len)>0 and len(fn_len)>0:\n","    t_stat2, t_p2 = stats.ttest_ind(fp_len, fn_len, equal_var=False)\n","    anova_stat2, anova_p2 = stats.f_oneway(fp_len, fn_len)\n","    chi2_h1, chi2_p2, _, _ = stats.chi2_contingency([np.histogram(fp_len, bins=8)[0]+1e-8, np.histogram(fn_len, bins=8)[0]+1e-8])\n","    mean_diff2 = fp_len.mean() - fn_len.mean()\n","    pooled_std2 = math.sqrt(fp_len.var()/len(fp_len) + fn_len.var()/len(fn_len))\n","    z_stat2 = mean_diff2 / pooled_std2\n","    z_p2 = 2*(1 - norm.cdf(abs(z_stat2)))\n","    print(\"FP vs FN p-values: t:\", t_p2, \"anova:\", anova_p2, \"chi2:\", chi2_p2, \"z:\", z_p2)\n","\n","    # log p-values\n","    pvals2 = [t_p2, anova_p2, chi2_p2, z_p2]\n","    logp2 = [-np.log10(p) if p>0 else 50 for p in pvals2]\n","    plt.figure(figsize=(6,3))\n","    sns.barplot(x=['T','ANOVA','Chi2','Z'], y=logp2, palette='mako')\n","    plt.title(\"FP vs FN -log10(p)\")\n","    plt.show()\n","else:\n","    print(\"No FP or no FN samples to compare (small test set).\")\n","\n","# ---------------------------\n","# END: save summary results\n","# ---------------------------\n","results_df.to_csv(\"ml_results_summary.csv\", index=False)\n","print(\"Pipeline finished. Results saved to ml_results_summary.csv\")\n"]}]}